---
title: "Lab 1: K-nearest neighbors"
author: "BST 263"
date: "February 2, 2023"
output:
  rmdformats::html_clean:
    thumbnails: true
    lightbox: true
    gallery: true
pkgdown:
  as_is: true 
---
```{r setup,include=F}
library(knitr)
library(rmdformats)
library(DT)
```

# Theoretical Problems

## Setup
Suppose $f(x)$ is a real-valued function on $\mathbb{R}^n$, i.e., $f(x)\in \mathbb{R}$ for $x \in \mathbb{R}^n$. The gradient of $f$, denoted $\nabla f(x)$, is the column vector in $\mathbb{R}^n$ with $k^{th}$ entry 
\[
(\nabla f(x))_k=\frac{\partial}{\partial x_k}f(x)
\]

The Hessian of $f$, denoted $\nabla^2 f(x)$, is the $n\times n$ matrix with $(k,l)^{th}$ entry
\[
(\nabla^2 f(x))_{kl}=\frac{\partial^2}{\partial x_k \partial x_l} f(x)
\]

&nbsp;

## Problem 1
If $A\in \mathbb{R}^{m \times n}$ and $f(x)=x^TA^TAx$, then what is $\nabla f(x)$?

Hint: $f(x)=\sum_i(\sum_j A_{ij}x_j)^2$

&nbsp;

## Problem 2
If $A\in \mathbb{R}^{m \times n}$ and $f(x)=x^TA^TAx$, then what is $\nabla^2 f(x)$?

&nbsp;

&nbsp;

# Coding Problems

## Problem 3
In lecture, we saw R code for a KNN classifier for univariate x's and binary y's (knn-classifier.r).
Please complete the functions below to extend this to a KNN classifier for multivariate x's and more than two classes. Recall that the Euclidean distance between two vectors $\mathbf{a}\in \mathbb{R}^n$ and $\mathbf{b}\in \mathbb{R}^n$ is $$dist(\mathbf{a},\mathbf{b})=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+...+(a_n-b_n)^2}$$


```{r p3, eval=FALSE}
# KNN classifier algorithm (multivariate, multiclass) -- probability version
#   x0 = length d vector, the new x point at which to predict the y value.
#   x = d-by-n matrix of training x's, where x[,i] is the i'th training point x_i.
#   y = (y[1],...,y[n]) = vector of training y's, where y[i] is in {1,...,C}.
#   K = number of neighbors to use.
#   C = number of classes.
#   p_hat = (p_hat[1],...,p_hat[C]) where p_hat[j] = estimated probability of y0=j given x0.

KNN_multi = function(x0, x, y, K, C) {
    # 1. Compute euclidean distance between x0 and each x[,i]
    distances <- sqrt(colSums((x-x0)^2))  
    # 2. order of the training points by distance from x0 (nearest to farthest)
    o <- order(distances)  
    # 3. p_hat[j] = proportion of y values of the K nearest training points that are equal to j.
    p_hat <- sapply(1:C, function(j){ sum(y[o[1:K]]==j)/K })  
    # 4. return estimated probabilities
    return(p_hat)  
}

# KNN classifier algorithm (multivariate, multiclass) -- prediction version
KNN_multi_predict = function(x0, x, y, K, C) {
    # 1. compute the estimated probabilities
    p_hat <- KNN_multi(x0, x, y, K, C)  
    # 2. find the class with the highest estimated probability
    y0 <- which.max(p_hat)  
    # 3. return the predicted class
    return(y0)  
}
```

&nbsp;

## Problem 4

Following the code below, simulate training and test datasets where the dimension of $x$, denoted $d$, is (a) $d=2$ and (b) $d=20$. For both cases (a) and (b), train your KNN classifier on the training set with K=5 and evaulate its predictive performance in the test set (note that since we have simulated the data, we know the true outcomes for the test set). Explain the intuition behind the difference in your classifier's performance in the two cases.

```{r p4,eval=F}
# Simulate training and test data

# reset the random number generator
set.seed(1) 
# dimension of each training point x_i- set this to (a) 2 and (b) 20
d = "SET THIS"
# number of training samples to simulate
n = 100  
# number of test samples to simulate
n_test = 10000  
# true relationship between x's and y's
f = function(x0) { (x0[1]>0)+(x0[2]>0)+1 }  
# simulate matrix of training x's
x = matrix(rnorm(n*d),d,n) 
 # simulate training y's with no noise
y = apply(x,2,f) 
# simulate test x's
x_test = matrix(rnorm(n_test*d),d,n_test)  
# simulate test y's
y_test = apply(x_test,2,f)  
# plot dimensions 1 and 2
plot(x[1,],x[2,],col=y+1,pch=19)  

# Apply your KNN classifier with K=5

# number of classes
C = 3 
# number of neighbors to use
K = 5  
# predictions on the training set
y_hat = apply(x, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })
# predictions on the test set
y_test_hat = apply(x_test, 2, function(x0) { KNN_multi_predict(x0, x, y, K, C) })  
# compute the training error rate
train_error = mean(y_hat != y)  
# compute the test error rate
test_error = mean(y_test_hat != y_test)  

print(data.frame('error type'=c('train','test'),'value'=c(train_error,test_error)))

```

&nbsp;

## Problem 5

##### a. Compute test error when we always naively predict y0=2, regardless of x0.
```{r p5a,eval=FALSE}
# Compute test error when we always naively predict y0=2, regardless of x0.
naive_test_error = mean(2 != y_test)
```

##### b. Compute test error when we make the optimal prediction f(x0).
```{r p5b,eval=FALSE}
# Compute test error when we make the optimal prediction f(x0).
optimal_test_error = mean(apply(x_test,2,f) != y_test)
```

##### c. Should the naive and optimal results depend on whether d=2 or d=20? Explain.

##### d. In 1-2 sentences, compare the performance of KNN when d=2 with KNN when d=20, relative to the naive and optimal performance.

##### e. Plot the KNN predictions for the first 100 test points when (a) d=2 and (b) d=20.  Briefly describe what you see.
```{r p5e,eval=FALSE}
# (Change d above and rerun the code for each case.)
plot(x_test[1,1:100],x_test[2,1:100],col=y_test_hat[1:100]+1,pch=19)  # show dims 1 and 2 only
```

&nbsp;

## Problem 6

In this problem, we will see how KNN can be used for cancer subtype classification.

Download the following two files from Files/Labs/Lab1 on Canvas:
  lab-1-gene-expression.txt
  lab-1-leukemia-type.txt

These files contain gene expression data for n=72 leukemia patients, along with the corresponding leukemia subtype (ALL, AML, or MLL).

This data is from:
  Armstrong et al., MLL translocations specify a distinct gene expression profile that 
  distinguishes a unique leukemia. Nature Genetics, 30(1):41, 2002.

```{r p6,eval=FALSE}
# Load the gene expression data
Dx = read.table(file="lab-1-gene-expression.txt", header=TRUE, sep='\t', row.names=1)
x = sapply(Dx, as.numeric)

# Load the cancer subtype labels
Dy = read.table(file="lab-1-leukemia-type.txt", header=TRUE, sep='\t', row.names=1)
y = as.numeric(factor(Dy[,1]))

# Split into training and test subsets
n = length(y)  # number of samples
set.seed(1)  # reset the random number generator
train = sample(1:n, round(n/2))  # random subset to use as training set
test = setdiff(1:n, train)  # subset to use as test set

# Run your KNN classifier with (a) K=5 and (b) K=30 to make predictions on the test set x[,test]
# based on the training data in x[,train] and y[train].  Compute the error rate on the test set by 
# comparing your predictions with y[test]. Report your results below, using the same train/test split
# for both K=5 and K=30.
C = 3  # number of classes
K = 5  # number of neighbors to use
y_test_hat = apply(x[,test], 2, function(x0) { "YOUR_ANSWER_HERE" })  # predictions on the test set
test_error = mean(y_test_hat != y[test])  # compute the error rate on the test set

# Report your results:
# (a) K=5
#    test error rate = YOUR_ANSWER_HERE
#
# (b) K=30
#    test error rate = YOUR_ANSWER_HERE
```
